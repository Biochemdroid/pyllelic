{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c9aeda3bcb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m##################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"Run a given set of Pyllelic analysis, using values from\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"pyllelic: a tool for detection of allelic-specific variation\n",
    "   in reduced representation bisulfate DNA sequencing.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Set, Union, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import pysam\n",
    "from Bio import pairwise2\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from . import quma\n",
    "# from .config import Config\n",
    "\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(filename=\"pyllelic_test.log\", level=logging.DEBUG)\n",
    "\n",
    "# # Initialize shared configuration object\n",
    "# config = Config()\n",
    "\n",
    "# # Initialized multiprocessing limits\n",
    "# NUM_THREADS = cpu_count() - 1\n",
    "\n",
    "\n",
    "# def set_up_env_variables(\n",
    "#     base_path: str,\n",
    "#     prom_file: str,\n",
    "#     prom_start: str,\n",
    "#     prom_end: str,\n",
    "#     chrom: str,\n",
    "#     offset: int,\n",
    "# ) -> None:\n",
    "#     \"\"\"Helper method to set up all our environmental variables, such as for testing.\n",
    "\n",
    "#     Args:\n",
    "#         base_path (str): directory where all processing will occur, put .bam files\n",
    "#                          in \"test\" sub-directory in this folder\n",
    "#         prom_file (str): filename of genmic sequence of promoter region of interest\n",
    "#         prom_start (str): start position to analyze in promoter region\n",
    "#         prom_end (str): final position to analyze in promoter region\n",
    "#         chrom (str): chromosome promoter is located on\n",
    "#         offset (int): genomic position of promoter to offset reads\n",
    "#     \"\"\"\n",
    "\n",
    "#     config.base_directory = Path(base_path)\n",
    "#     config.promoter_file = Path(base_path) / prom_file\n",
    "#     config.results_directory = Path(base_path) / \"results\"\n",
    "#     config.bam_directory = Path(base_path) / \"bam_output\"\n",
    "#     config.analysis_directory = Path(base_path) / \"test\"\n",
    "#     config.promoter_start = prom_start\n",
    "#     config.promoter_end = prom_end\n",
    "#     config.chromosome = chrom\n",
    "#     config.offset = offset\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "import config\n",
    "def main() -> None:\n",
    "    \"\"\"Run a given set of Pyllelic analysis, using values from\n",
    "    supplied environmental variables.\n",
    "    \"\"\"\n",
    "\n",
    "    if sys.argv[1]:\n",
    "        filename: str = sys.argv[1]\n",
    "    else:  # pragma: no cover\n",
    "        filename = \"output.xlsx\"\n",
    "\n",
    "    files_set: List[str] = make_list_of_bam_files()\n",
    "    positions: List[str] = index_and_fetch(files_set)\n",
    "    genome_parsing()\n",
    "    cell_types: List[str] = extract_cell_types(files_set)\n",
    "    df_list: Dict[str, pd.DataFrame] = run_quma_and_compile_list_of_df(\n",
    "        cell_types, filename\n",
    "    )\n",
    "    means_df: pd.DataFrame = process_means(df_list, positions, cell_types)\n",
    "    modes_df: pd.DataFrame = process_modes(df_list, positions, cell_types)\n",
    "    diffs_df: pd.DataFrame = find_diffs(means_df, modes_df)\n",
    "    write_means_modes_diffs(means_df, modes_df, diffs_df, filename)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def genome_range(\n",
    "    position: str, genome_string: str, offset: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"Helper to return a genome string (e.g., \"ATCGACTAG\")\n",
    "    given a position and an entire string.\n",
    "\n",
    "    Args:\n",
    "        position: genomic position on chromesome\n",
    "        genome_string: string representation of genomic promoter known sequence\n",
    "        offset: genomic position of promoter to offset reads\n",
    "\n",
    "    Returns:\n",
    "        str: genomic bases for indicated read / position\n",
    "    \"\"\"\n",
    "\n",
    "    # OFFSET: int = 1298163  # TERT offset\n",
    "\n",
    "    if not offset:\n",
    "        offset = config.offset\n",
    "\n",
    "    start: int = offset - (int(position) + 30)\n",
    "    end: int = offset - (int(position) + 1)\n",
    "\n",
    "    return genome_string[start:end]\n",
    "\n",
    "\n",
    "def make_list_of_bam_files() -> List[str]:\n",
    "    \"\"\"Check analysis directory for all valid .bam files.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of files\n",
    "    \"\"\"\n",
    "    return [f.name for f in config.analysis_directory.iterdir() if f.suffix == \".bam\"]\n",
    "\n",
    "\n",
    "def index_and_fetch(files_set: List[str], process: bool = True) -> List[str]:\n",
    "    \"\"\"Wrapper to call processing of each sam file.\n",
    "\n",
    "    Args:\n",
    "        files_set (list[str]): list of bam/sam files\n",
    "        process (bool): process and write files flag. Default True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of genomic positions analyzed\n",
    "    \"\"\"\n",
    "\n",
    "    sam_path: List[Path] = [config.base_directory / \"test\" / f for f in files_set]\n",
    "\n",
    "    all_pos: Set = set()\n",
    "    for sams in tqdm(sam_path):\n",
    "        pos: pd.Index = run_sam_and_extract_df(sams, process)\n",
    "        all_pos.update(pos)\n",
    "\n",
    "    return sorted(list(all_pos))\n",
    "\n",
    "\n",
    "def run_sam_and_extract_df(sams: Path, process: bool = True) -> pd.Index:\n",
    "    \"\"\"Process samfiles, pulling out sequence and position data\n",
    "    and writing to folders/files.\n",
    "\n",
    "    Args:\n",
    "        sams (Path): path to a samfile\n",
    "        process (bool): process and write files flag. Default True.\n",
    "\n",
    "    Returns:\n",
    "        pd.Index: list of unique positions in the samfile\n",
    "    \"\"\"\n",
    "\n",
    "    # Index samfile if index file doesn't exist\n",
    "    if not sams.with_suffix(\".bai\").exists():\n",
    "        _: bool = pysam_index(sams)  # we don't care what the output is\n",
    "\n",
    "    # Grab the promoter region of interest\n",
    "    samm: pysam.AlignmentFile = pysam.AlignmentFile(sams, \"rb\")\n",
    "    itern = samm.fetch(\n",
    "        config.chromosome, int(config.promoter_start), int(config.promoter_end)\n",
    "    )\n",
    "\n",
    "    position: List = []\n",
    "    sequence: List = []\n",
    "\n",
    "    for x in itern:\n",
    "        cols = str(x).split()\n",
    "        position.append(cols[3])\n",
    "        sequence.append(cols[9])\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(\n",
    "        list(zip(position, sequence)), columns=[\"positions\", \"sequence\"]\n",
    "    )\n",
    "\n",
    "    df2: pd.DataFrame = df.set_index(\"positions\")\n",
    "    # will set the inital index (on the leftmost column) to be position\n",
    "    df3: pd.DataFrame = df2.stack()\n",
    "    # if confused, see: https://www.w3resource.com/pandas/dataframe/dataframe-stack.php\n",
    "\n",
    "    if process:\n",
    "        write_bam_output_files(sams, df2.index.unique(), df3)\n",
    "\n",
    "    return df2.index.unique()\n",
    "\n",
    "\n",
    "def write_bam_output_files(sams: Path, positions: List[str], df: pd.DataFrame) -> None:\n",
    "    \"\"\"Extract alignments from sequencing reads and output text files\n",
    "       in bam directory.\n",
    "\n",
    "    Args:\n",
    "        sams (Path): path to a sam file\n",
    "        positions (List[str]): list of unique positions\n",
    "        df (pd.DataFrame): dataframe of sequencing reads\n",
    "    \"\"\"\n",
    "\n",
    "    sam_name: str = sams.name\n",
    "\n",
    "    # Make sure bam_output directory and sam subdirectories exist\n",
    "    config.base_directory.joinpath(\"bam_output\", sam_name).mkdir(\n",
    "        parents=True, exist_ok=True\n",
    "    )\n",
    "    for each1 in positions:\n",
    "        alignments: List = []\n",
    "\n",
    "        # # Set up query using alignment algorithm\n",
    "        # query_sequence: List[str] = df.loc[each1].head(1).tolist()[0]\n",
    "        # query: StripedSmithWaterman = StripedSmithWaterman(query_sequence)\n",
    "\n",
    "        # # Set up sequences to check for alignment\n",
    "        # target_sequences: List[str] = df.loc[each1].tolist()\n",
    "        # for target_sequence in target_sequences:\n",
    "        #     alignment = query(target_sequence)\n",
    "        #     alignments.append(alignment)\n",
    "\n",
    "        # read_file: List[str] = []\n",
    "        # for index, each in enumerate(alignments):\n",
    "        #     read_file.append(str(\">read\" + str(index)))\n",
    "        #     read_file.append(each.aligned_target_sequence)\n",
    "        #     # returns aligned target sequence\n",
    "\n",
    "        # Set up query using alignment algorithm\n",
    "        query_sequence: List[str] = df.loc[each1].head(1).tolist()[0]\n",
    "\n",
    "        # Set up sequences to check for alignment\n",
    "        target_sequences: List[str] = df.loc[each1].tolist()\n",
    "        for target_sequence in target_sequences:\n",
    "            alignment = pairwise2.align.localms(\n",
    "                query_sequence, target_sequence, 2, -3, -5, -2\n",
    "            )\n",
    "            aligned_segment = alignment[0].seqA[alignment[0].start : alignment[0].end]\n",
    "            alignments.append(aligned_segment)\n",
    "\n",
    "        read_file: List[str] = []\n",
    "        for index, each in enumerate(alignments):\n",
    "            read_file.append(str(\">read\" + str(index)))\n",
    "            read_file.append(each)\n",
    "            # returns aligned target sequence\n",
    "\n",
    "        write_individual_bam_file(sam_name, each1, read_file)\n",
    "        print(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}